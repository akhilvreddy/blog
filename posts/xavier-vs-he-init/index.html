<!DOCTYPE html>
<html lang="en">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="üß† The Problem When training neural nets, bad weight initialization leads to exploding/vanishing gradients.
üìê Xavier Initialization Designed for tanh / sigmoid Keeps variance consistent across layers Formula:
$$ W \sim \mathcal{U}\left(-\frac{\sqrt{6}}{\sqrt{n_{in} &#43; n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} &#43; n_{out}}} \right) $$
‚ö° He Initialization Designed for ReLU Keeps forward activation variance high enough Formula:
$$ W \sim \mathcal{N}\left(0, \frac{2}{n_{in}} \right) $$
üß™ PyTorch Example import torch.nn as nn # Xavier nn.Linear(256, 128) nn.init.xavier_uniform_(layer.weight) # He nn.init.kaiming_normal_(layer.weight, nonlinearity=&#39;relu&#39;) " />
<meta name="keywords" content=", untagged" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="http://localhost:1313/posts/xavier-vs-he-init/" />


    <title>
        
            Xavier vs He Init :: My New Hugo Site 
        
    </title>





  <link rel="stylesheet" href="/main.min.07ea7ac7da67e2e153a7dfa2457bc6a19cca824288d175e223fadc579041bc51.css" integrity="sha256-B&#43;p6x9pn4uFTp9&#43;iRXvGoZzKgkKI0XXiI/rcV5BBvFE=" crossorigin="anonymous">





    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="Xavier vs He Init">
  <meta itemprop="description" content="üß† The Problem When training neural nets, bad weight initialization leads to exploding/vanishing gradients.
üìê Xavier Initialization Designed for tanh / sigmoid Keeps variance consistent across layers Formula:
$$ W \sim \mathcal{U}\left(-\frac{\sqrt{6}}{\sqrt{n_{in} &#43; n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} &#43; n_{out}}} \right) $$
‚ö° He Initialization Designed for ReLU Keeps forward activation variance high enough Formula:
$$ W \sim \mathcal{N}\left(0, \frac{2}{n_{in}} \right) $$
üß™ PyTorch Example import torch.nn as nn # Xavier nn.Linear(256, 128) nn.init.xavier_uniform_(layer.weight) # He nn.init.kaiming_normal_(layer.weight, nonlinearity=&#39;relu&#39;)">
  <meta itemprop="datePublished" content="2025-06-20T11:01:04-04:00">
  <meta itemprop="dateModified" content="2025-06-20T11:01:04-04:00">
  <meta itemprop="wordCount" content="75">
  <meta itemprop="keywords" content="Untagged">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Xavier vs He Init">
  <meta name="twitter:description" content="üß† The Problem When training neural nets, bad weight initialization leads to exploding/vanishing gradients.
üìê Xavier Initialization Designed for tanh / sigmoid Keeps variance consistent across layers Formula:
$$ W \sim \mathcal{U}\left(-\frac{\sqrt{6}}{\sqrt{n_{in} &#43; n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} &#43; n_{out}}} \right) $$
‚ö° He Initialization Designed for ReLU Keeps forward activation variance high enough Formula:
$$ W \sim \mathcal{N}\left(0, \frac{2}{n_{in}} \right) $$
üß™ PyTorch Example import torch.nn as nn # Xavier nn.Linear(256, 128) nn.init.xavier_uniform_(layer.weight) # He nn.init.kaiming_normal_(layer.weight, nonlinearity=&#39;relu&#39;)">







    <meta property="article:published_time" content="2025-06-20 11:01:04 -0400 EDT" />












    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                hello</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        One minute

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="http://localhost:1313/posts/xavier-vs-he-init/">Xavier vs He Init</a>
      </h1>

      

      

      

      <div class="post-content">
        <h2 id="-the-problem">üß† The Problem</h2>
<p>When training neural nets, bad weight initialization leads to exploding/vanishing gradients.</p>
<h2 id="-xavier-initialization">üìê Xavier Initialization</h2>
<ul>
<li>Designed for tanh / sigmoid</li>
<li>Keeps variance consistent across layers</li>
</ul>
<p><strong>Formula:</strong></p>
<p>$$ W \sim \mathcal{U}\left(-\frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}} \right) $$</p>
<h2 id="-he-initialization">‚ö° He Initialization</h2>
<ul>
<li>Designed for ReLU</li>
<li>Keeps forward activation variance high enough</li>
</ul>
<p><strong>Formula:</strong></p>
<p>$$ W \sim \mathcal{N}\left(0, \frac{2}{n_{in}} \right) $$</p>
<h2 id="-pytorch-example">üß™ PyTorch Example</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Xavier</span>
</span></span><span style="display:flex;"><span>nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform_(layer<span style="color:#f92672">.</span>weight)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># He</span>
</span></span><span style="display:flex;"><span>nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>kaiming_normal_(layer<span style="color:#f92672">.</span>weight, nonlinearity<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)
</span></span></code></pre></div>
      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="http://localhost:1313/tags/untagged/">untagged</a></span>
        
    </p>

      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        75 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2025-06-20 11:01
        

         
          
        
      </p>
    </div>

    
    <div class="pagination">
        

        <div class="pagination__buttons">
            
            <span class="button previous">
                <a href="http://localhost:1313/posts/migrate-from-jekyll/">
                    <span class="button__icon">‚Üê</span>
                    <span class="button__text">Migrate to Hugo from Jekyll</span>
                </a>
            </span>
            

            
            <span class="button next">
                <a href="http://localhost:1313/posts/creation-dun-nouveau-theme/">
                    <span class="button__text">Cr√©ation d&#39;un nouveau th√®me</span>
                    <span class="button__icon">‚Üí</span>
                </a>
            </span>
            
        </div>
    </div>


    

    

    

  </main>

            </div>

            
                <footer class="footer">
    
    
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.ad54ad97364f77ede35def9096b162bb1f0b3973aa50b080f5e82fa147f6882e2a7200d7535adbf9b51bebf939f1c1ca9bbe6be87530092aca720eac4a226fda.js" integrity="sha512-rVStlzZPd&#43;3jXe&#43;QlrFiux8LOXOqULCA9egvoUf2iC4qcgDXU1rb&#43;bUb6/k58cHKm75r6HUwCSrKcg6sSiJv2g=="></script>




    </body>
</html>
