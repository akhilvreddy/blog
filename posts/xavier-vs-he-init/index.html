<!doctype html><html lang=en><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="🧠 The Problem When training neural nets, bad weight initialization leads to exploding/vanishing gradients.
📐 Xavier Initialization Designed for tanh / sigmoid Keeps variance consistent across layers Formula:
$$ W \sim \mathcal{U}\left(-\frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}} \right) $$
⚡ He Initialization Designed for ReLU Keeps forward activation variance high enough Formula:
$$ W \sim \mathcal{N}\left(0, \frac{2}{n_{in}} \right) $$
🧪 PyTorch Example import torch.nn as nn # Xavier nn.Linear(256, 128) nn.init.xavier_uniform_(layer.weight) # He nn.init.kaiming_normal_(layer.weight, nonlinearity='relu') "><meta name=keywords content=",untagged"><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://example.org/posts/xavier-vs-he-init/><title>Xavier vs He Init :: My New Hugo Site</title><link rel=stylesheet href=/main.min.07ea7ac7da67e2e153a7dfa2457bc6a19cca824288d175e223fadc579041bc51.css integrity="sha256-B+p6x9pn4uFTp9+iRXvGoZzKgkKI0XXiI/rcV5BBvFE=" crossorigin=anonymous><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color><link rel="shortcut icon" href=/favicon.ico><meta name=msapplication-TileColor content><meta itemprop=name content="Xavier vs He Init"><meta itemprop=description content="🧠 The Problem When training neural nets, bad weight initialization leads to exploding/vanishing gradients.
📐 Xavier Initialization Designed for tanh / sigmoid Keeps variance consistent across layers Formula:
$$ W \sim \mathcal{U}\left(-\frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}} \right) $$
⚡ He Initialization Designed for ReLU Keeps forward activation variance high enough Formula:
$$ W \sim \mathcal{N}\left(0, \frac{2}{n_{in}} \right) $$
🧪 PyTorch Example import torch.nn as nn # Xavier nn.Linear(256, 128) nn.init.xavier_uniform_(layer.weight) # He nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"><meta itemprop=datePublished content="2025-06-20T11:01:04-04:00"><meta itemprop=dateModified content="2025-06-20T11:01:04-04:00"><meta itemprop=wordCount content="75"><meta itemprop=keywords content="Untagged"><meta name=twitter:card content="summary"><meta name=twitter:title content="Xavier vs He Init"><meta name=twitter:description content="🧠 The Problem When training neural nets, bad weight initialization leads to exploding/vanishing gradients.
📐 Xavier Initialization Designed for tanh / sigmoid Keeps variance consistent across layers Formula:
$$ W \sim \mathcal{U}\left(-\frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}} \right) $$
⚡ He Initialization Designed for ReLU Keeps forward activation variance high enough Formula:
$$ W \sim \mathcal{N}\left(0, \frac{2}{n_{in}} \right) $$
🧪 PyTorch Example import torch.nn as nn # Xavier nn.Linear(256, 128) nn.init.xavier_uniform_(layer.weight) # He nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')"><meta property="article:published_time" content="2025-06-20 11:01:04 -0400 -0400"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo><span class=logo__mark>></span>
<span class=logo__text>hello</span>
<span class=logo__cursor></span></div></a><span class=header__right></span></span></header><div class=content><main class=post><div class=post-info><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
One minute</p></div><article><h1 class=post-title><a href=https://example.org/posts/xavier-vs-he-init/>Xavier vs He Init</a></h1><div class=post-content><h2 id=-the-problem>🧠 The Problem</h2><p>When training neural nets, bad weight initialization leads to exploding/vanishing gradients.</p><h2 id=-xavier-initialization>📐 Xavier Initialization</h2><ul><li>Designed for tanh / sigmoid</li><li>Keeps variance consistent across layers</li></ul><p><strong>Formula:</strong></p><p>$$ W \sim \mathcal{U}\left(-\frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}} \right) $$</p><h2 id=-he-initialization>⚡ He Initialization</h2><ul><li>Designed for ReLU</li><li>Keeps forward activation variance high enough</li></ul><p><strong>Formula:</strong></p><p>$$ W \sim \mathcal{N}\left(0, \frac{2}{n_{in}} \right) $$</p><h2 id=-pytorch-example>🧪 PyTorch Example</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Xavier</span>
</span></span><span style=display:flex><span>nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>128</span>)
</span></span><span style=display:flex><span>nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>xavier_uniform_(layer<span style=color:#f92672>.</span>weight)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># He</span>
</span></span><span style=display:flex><span>nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>kaiming_normal_(layer<span style=color:#f92672>.</span>weight, nonlinearity<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span></code></pre></div></div></article><hr><div class=post-info><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg>
<span class=tag><a href=https://example.org/tags/untagged/>untagged</a></span></p><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>
75 Words</p><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
2025-06-20 15:01</p></div><div class=pagination><div class=pagination__buttons><span class="button previous"><a href=https://example.org/posts/migrate-from-jekyll/><span class=button__icon>←</span>
<span class=button__text>Migrate to Hugo from Jekyll</span>
</a></span><span class="button next"><a href=https://example.org/posts/creation-dun-nouveau-theme/><span class=button__text>Création d'un nouveau thème</span>
<span class=button__icon>→</span></a></span></div></div></main></div><footer class=footer></footer></div><script type=text/javascript src=/bundle.min.ad54ad97364f77ede35def9096b162bb1f0b3973aa50b080f5e82fa147f6882e2a7200d7535adbf9b51bebf939f1c1ca9bbe6be87530092aca720eac4a226fda.js integrity="sha512-rVStlzZPd+3jXe+QlrFiux8LOXOqULCA9egvoUf2iC4qcgDXU1rb+bUb6/k58cHKm75r6HUwCSrKcg6sSiJv2g=="></script></body></html>